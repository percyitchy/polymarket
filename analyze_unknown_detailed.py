#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Unknown —Ä—ã–Ω–∫–æ–≤ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –¥–æ 20%
"""

import os
import sys
import logging
from collections import Counter, defaultdict
from dotenv import load_dotenv
from db import PolymarketDB
import requests

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def analyze_unknown_detailed():
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Unknown —Ä—ã–Ω–∫–æ–≤"""
    db_path = os.getenv('DB_PATH', 'polymarket_notifier.db')
    if not os.path.isabs(db_path):
        db_path = os.path.abspath(db_path)
    
    db = PolymarketDB(db_path)
    
    print("=" * 80)
    print("üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó UNKNOWN –†–´–ù–ö–û–í")
    print("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º Unknown —Ä—ã–Ω–∫–∏ –∏–∑ –±–∞–∑—ã
    with db.get_connection() as conn:
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ—à–µ–ª—å–∫–∏ —Å Unknown –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        cursor.execute("""
            SELECT DISTINCT wallet_address
            FROM wallet_category_stats
            WHERE category = 'other/Unknown'
            LIMIT 200
        """)
        
        unknown_wallets = [row[0] for row in cursor.fetchall()]
        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ {len(unknown_wallets)} –∫–æ—à–µ–ª—å–∫–æ–≤ —Å Unknown –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã Unknown —Ä—ã–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ API
        print("\nüì• –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä—ã–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ API...")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—ã—Ç–∏—è
        url = "https://gamma-api.polymarket.com/events"
        params = {"limit": 300, "featured": "true"}
        
        try:
            response = requests.get(url, params=params, timeout=15)
            if response.status_code == 200:
                data = response.json()
                events = []
                if isinstance(data, list):
                    events = data
                elif isinstance(data, dict):
                    events = data.get("data") or data.get("events") or []
                
                print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(events)} —Å–æ–±—ã—Ç–∏–π")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ä—ã–Ω–∫–∏
                all_markets = []
                for event in events:
                    markets = event.get("markets", [])
                    for market in markets:
                        market["event"] = event
                        all_markets.append(market)
                
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_markets)} —Ä—ã–Ω–∫–æ–≤")
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∏ –Ω–∞—Ö–æ–¥–∏–º Unknown
                from market_utils import classify_market
                
                unknown_samples = []
                classified_samples = []
                
                for market in all_markets[:500]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    condition_id = market.get("conditionId") or market.get("condition_id")
                    slug = market.get("slug") or market.get("marketSlug") or ""
                    question = market.get("question") or market.get("title") or ""
                    description = market.get("description") or ""
                    
                    event = market.get("event", {})
                    
                    category = classify_market(event, slug, question or description)
                    
                    market_info = {
                        "condition_id": condition_id,
                        "slug": slug,
                        "question": question,
                        "description": description,
                        "category": category,
                        "full_text": f"{slug} {question} {description}".lower()
                    }
                    
                    if category == "other/Unknown":
                        unknown_samples.append(market_info)
                    else:
                        classified_samples.append(market_info)
                
                print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
                print(f"   –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {len(classified_samples)}")
                print(f"   Unknown: {len(unknown_samples)}")
                print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç Unknown: {len(unknown_samples) / len(all_markets[:500]) * 100:.2f}%")
                
                if unknown_samples:
                    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ {len(unknown_samples)} Unknown —Ä—ã–Ω–∫–æ–≤:")
                    
                    # –ê–Ω–∞–ª–∏–∑ –ø–æ –Ω–∞–ª–∏—á–∏—é –¥–∞–Ω–Ω—ã—Ö
                    has_slug = sum(1 for m in unknown_samples if m["slug"])
                    has_question = sum(1 for m in unknown_samples if m["question"])
                    has_description = sum(1 for m in unknown_samples if m["description"])
                    has_any = sum(1 for m in unknown_samples if m["slug"] or m["question"] or m["description"])
                    empty = len(unknown_samples) - has_any
                    
                    print(f"\nüìã –ù–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
                    print(f"   –ï—Å—Ç—å slug: {has_slug} ({has_slug/len(unknown_samples)*100:.1f}%)")
                    print(f"   –ï—Å—Ç—å question: {has_question} ({has_question/len(unknown_samples)*100:.1f}%)")
                    print(f"   –ï—Å—Ç—å description: {has_description} ({has_description/len(unknown_samples)*100:.1f}%)")
                    print(f"   –ï—Å—Ç—å –ª—é–±—ã–µ –¥–∞–Ω–Ω—ã–µ: {has_any} ({has_any/len(unknown_samples)*100:.1f}%)")
                    print(f"   –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ: {empty} ({empty/len(unknown_samples)*100:.1f}%)")
                    
                    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
                    import re
                    
                    patterns_analysis = {
                        "dates": [],
                        "numbers": [],
                        "short_text": [],
                        "common_words": []
                    }
                    
                    keywords = Counter()
                    stop_words = {
                        "will", "the", "be", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
                        "of", "with", "by", "from", "as", "is", "are", "was", "were", "been", "being"
                    }
                    
                    for market in unknown_samples:
                        text = market["full_text"]
                        
                        # –î–∞—Ç—ã
                        if re.search(r'\d{4}-\d{2}-\d{2}|\d{1,2}/\d{1,2}/\d{4}|january|february|march|april|may|june|july|august|september|october|november|december', text, re.IGNORECASE):
                            patterns_analysis["dates"].append(market)
                        
                        # –ß–∏—Å–ª–∞
                        if re.search(r'\$[\d,]+|\d+%|\d+\.\d+%|\d+[km]?', text):
                            patterns_analysis["numbers"].append(market)
                        
                        # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                        if len(text) < 30:
                            patterns_analysis["short_text"].append(market)
                        
                        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                        words = text.split()
                        for word in words:
                            word_clean = word.strip(".,!?;:()[]{}'\"-").lower()
                            if len(word_clean) > 3 and word_clean not in stop_words:
                                keywords[word_clean] += 1
                    
                    print(f"\nüî§ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≤ Unknown:")
                    print(f"   –° –¥–∞—Ç–∞–º–∏: {len(patterns_analysis['dates'])} ({len(patterns_analysis['dates'])/len(unknown_samples)*100:.1f}%)")
                    print(f"   –° —á–∏—Å–ª–∞–º–∏: {len(patterns_analysis['numbers'])} ({len(patterns_analysis['numbers'])/len(unknown_samples)*100:.1f}%)")
                    print(f"   –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {len(patterns_analysis['short_text'])} ({len(patterns_analysis['short_text'])/len(unknown_samples)*100:.1f}%)")
                    
                    print(f"\nüìù –¢–æ–ø-20 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ Unknown:")
                    for word, count in keywords.most_common(20):
                        print(f"   {word:<25} {count:>4}")
                    
                    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã Unknown —Ä—ã–Ω–∫–æ–≤:")
                    print(f"\n1. –†—ã–Ω–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ Unknown:")
                    samples_with_data = [m for m in unknown_samples if m["slug"] or m["question"]][:10]
                    for i, market in enumerate(samples_with_data[:5], 1):
                        print(f"   {i}. Slug: {market['slug'][:60] if market['slug'] else 'N/A'}")
                        print(f"      Question: {market['question'][:80] if market['question'] else 'N/A'}")
                    
                    print(f"\n2. –†—ã–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏:")
                    for i, market in enumerate(patterns_analysis["dates"][:5], 1):
                        print(f"   {i}. {market['question'][:80] or market['slug'][:80]}")
                    
                    print(f"\n3. –†—ã–Ω–∫–∏ —Å —á–∏—Å–ª–∞–º–∏:")
                    for i, market in enumerate(patterns_analysis["numbers"][:5], 1):
                        print(f"   {i}. {market['question'][:80] or market['slug'][:80]}")
                    
                    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    print("\n" + "=" * 80)
                    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –°–ù–ò–ñ–ï–ù–ò–Ø UNKNOWN –î–û 20%:")
                    print("=" * 80)
                    
                    recommendations = []
                    
                    if empty > len(unknown_samples) * 0.3:
                        recommendations.append("1. ‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ù–û: –ú–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö - —É–ª—É—á—à–∏—Ç—å –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Å–µ API")
                    
                    if len(patterns_analysis["dates"]) > 0:
                        recommendations.append("2. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ –¥–∞—Ç–∞–º (—Å–æ–±—ã—Ç–∏—è, –¥–µ–¥–ª–∞–π–Ω—ã ‚Üí macro/Events)")
                    
                    if len(patterns_analysis["numbers"]) > 0:
                        recommendations.append("3. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ —Ü–µ–Ω–∞–º/–ø—Ä–æ—Ü–µ–Ω—Ç–∞–º (–º–∞–∫—Ä–æ, –∫—Ä–∏–ø—Ç–æ)")
                    
                    if len(patterns_analysis["short_text"]) > len(unknown_samples) * 0.2:
                        recommendations.append("4. ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ML –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ)")
                    
                    # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    top_keywords_list = [w for w, c in keywords.most_common(30) if c >= 2]
                    if top_keywords_list:
                        recommendations.append(f"5. üìù –î–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(top_keywords_list[:15])}")
                    
                    recommendations.append("6. ü§ñ –°–Ω–∏–∑–∏—Ç—å ML –ø–æ—Ä–æ–≥ –¥–æ 0.05 –¥–ª—è –æ—á–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                    recommendations.append("7. üîÑ –î–æ–±–∞–≤–∏—Ç—å fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–¥–∞—Ç—ã ‚Üí macro, —á–∏—Å–ª–∞ ‚Üí crypto/macro)")
                    recommendations.append("8. üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å event.category –∏–∑ API –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ")
                    recommendations.append("9. üéØ –î–æ–±–∞–≤–∏—Ç—å —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º")
                    recommendations.append("10. üíæ –£–ª—É—á—à–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ä—ã–Ω–∫–æ–≤")
                    
                    for rec in recommendations:
                        print(f"   {rec}")
                    
                    print("\n" + "=" * 80)
                    
                    return {
                        "total": len(all_markets[:500]),
                        "unknown": len(unknown_samples),
                        "unknown_pct": len(unknown_samples) / len(all_markets[:500]) * 100 if all_markets else 0,
                        "empty_data": empty,
                        "empty_pct": empty / len(unknown_samples) * 100 if unknown_samples else 0,
                        "patterns": patterns_analysis,
                        "top_keywords": dict(keywords.most_common(30))
                    }
                else:
                    print("‚úÖ –í—Å–µ —Ä—ã–Ω–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã!")
                    return None
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ API: {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}")
            return None

if __name__ == "__main__":
    results = analyze_unknown_detailed()
    if results:
        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω.")
        print(f"   Unknown: {results['unknown']} ({results['unknown_pct']:.2f}%)")
        print(f"   –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ: {results['empty_data']} ({results['empty_pct']:.1f}%)")

