#!/usr/bin/env python3
"""
HashDive Insiders Scraper using undetected-chromedriver
This bypasses Google's automation detection
"""

import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime


def scrape_hashdive():
    """Scrape HashDive Insiders page with undetected chromedriver"""
    
    print("=" * 60)
    print("HashDive Insiders Scraper (Undetected ChromeDriver)")
    print("=" * 60)
    print("\nüìã –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:")
    print("1. –ë—Ä–∞—É–∑–µ—Ä –æ—Ç–∫—Ä–æ–µ—Ç—Å—è —Å —Å–∞–π—Ç–æ–º HashDive")
    print("2. –í–æ–π–¥–∏—Ç–µ —á–µ—Ä–µ–∑ Google OAuth")
    print("   (undetected-chromedriver –æ–±—ã—á–Ω–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è)")
    print("3. –ü–æ—Å–ª–µ –ª–æ–≥–∏–Ω–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ")
    print("4. –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è")
    print("5. –°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç –¥–∞–Ω–Ω—ã–µ")
    print("\n‚è≥ –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–ø—É—Å–∫–∞...")
    input()
    
    # Setup undetected chromedriver
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    
    driver = uc.Chrome(options=options)
    
    try:
        print("\nüöÄ –û—Ç–∫—Ä—ã–≤–∞—é HashDive...")
        driver.get("https://hashdive.com")
        time.sleep(3)
        
        driver.save_screenshot("01_before_login.png")
        print("‚úì Screenshot: 01_before_login.png")
        
        # Check if login needed
        page_text = driver.page_source
        
        if 'Log in' in page_text or 'Log in' in page_text or 'log in' in page_text.lower():
            print("\n‚ö†Ô∏è  –í–´ –ù–ï –ó–ê–õ–û–ì–ò–ù–ï–ù–´")
        else:
            print("\n‚úÖ –í—ã –≤–æ–∑–º–æ–∂–Ω–æ —É–∂–µ –∑–∞–ª–æ–≥–∏–Ω–µ–Ω—ã")
        
        print("=" * 60)
        print("–í–ê–ñ–ù–û! –î–ê–ñ–ï –ï–°–õ–ò –ö–ê–ñ–ï–¢–°–Ø –ß–¢–û –í–´ –í–û–®–õ–ò:")
        print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –µ—Å—Ç—å –ª–∏ –∫–Ω–æ–ø–∫–∞ 'Log in' –≤ –ø—Ä–∞–≤–æ–º –≤–µ—Ä—Ö–Ω–µ–º —É–≥–ª—É")
        print("2. –ï—Å–ª–∏ –ï–°–¢–¨ - –Ω–∞–∂–º–∏—Ç–µ –∏ –≤–æ–π–¥–∏—Ç–µ —á–µ—Ä–µ–∑ Google")
        print("3. –ï—Å–ª–∏ –ù–ï–¢ - –æ—Ç–ª–∏—á–Ω–æ, –∑–Ω–∞—á–∏—Ç —É–∂–µ –∑–∞–ª–æ–≥–∏–Ω–µ–Ω—ã!")
        print("4. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –ø–æ–∫–∞ –≤—Å–µ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é")
        print("5. –ù–ï –ó–ê–ö–†–´–í–ê–ô–¢–ï –±—Ä–∞—É–∑–µ—Ä!")
        print("=" * 60)
        print("\n‚è≥ –û–∂–∏–¥–∞—é 120 —Å–µ–∫—É–Ω–¥ –Ω–∞ –≤–∞—à –ª–æ–≥–∏–Ω...")
        print("   –ü–æ—Å–ª–µ –ª–æ–≥–∏–Ω–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ –µ—â—ë 15 —Å–µ–∫—É–Ω–¥")
        print("   —á—Ç–æ–±—ã –≤—Å–µ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å –ø–æ–ª–Ω–æ—Å—Ç—å—é")
        time.sleep(120)
        print("‚úì –ü—Ä–æ–≤–µ—Ä—è—é —Å–æ—Å—Ç–æ—è–Ω–∏–µ...")
        
        # Check login status
        driver.save_screenshot("02_check_login.png")
        
        print("\nüåä –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É Insiders...")
        print("   –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è!")
        driver.get("https://hashdive.com/Insiders")
        print("   ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è... –∂–¥—É 10 —Å–µ–∫—É–Ω–¥")
        time.sleep(10)
        
        # Scroll down to load more content
        print("üìú –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
        for i in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        
        print("   –ñ–¥—É –µ—â—ë 5 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏...")
        time.sleep(5)
        
        driver.save_screenshot("03_insiders_page.png")
        print("‚úì Screenshot: 03_insiders_page.png")
        
        # Get page source
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract data
        print("\nüìä –ò–∑–≤–ª–µ–∫–∞—é –¥–∞–Ω–Ω—ã–µ...")
        data = {
            "timestamp": datetime.now().isoformat(),
            "tables": [],
            "all_text": soup.get_text()[:3000],
            "links": []
        }
        
        # Find all tables and data-containers
        tables = soup.find_all('table')
        dataframes = soup.find_all('div', class_=lambda x: x and 'dataframe' in x.lower())
        
        print(f"Found {len(tables)} tables, {len(dataframes)} dataframes")
        
        # Combine all data containers
        all_containers = list(tables) + list(dataframes)
        
        for idx, table in enumerate(all_containers):
            table_data = {
                "index": idx,
                "headers": [],
                "rows": []
            }
            
            # Try to get headers from thead
            thead = table.find('thead')
            if thead:
                headers = thead.find_all('th')
                table_data["headers"] = [h.get_text(strip=True) for h in headers]
            
            # Get rows from tbody or all rows if no tbody
            # For div dataframes, use different approach
            is_div = table.name == 'div'
            
            if is_div:
                # Streamlit dataframe structure
                rows = table.find_all('div', {'role': 'row'})
                if not rows:
                    rows = table.find_all(['div', 'span'])
            else:
                tbody = table.find('tbody')
                rows = tbody.find_all('tr') if tbody else table.find_all('tr')
            
            for row in rows[:50]:  # Limit to first 50 rows
                # For div-based structures, look for different cell types
                if is_div:
                    cells = row.find_all(['div', 'span'], recursive=False)
                else:
                    cells = row.find_all(['td', 'th'])
                
                row_data = []
                for cell in cells:
                    text = cell.get_text(strip=True)
                    # Check for links
                    links = []
                    for a in cell.find_all('a', href=True):
                        links.append({
                            "text": a.get_text(strip=True),
                            "href": a['href']
                        })
                    row_data.append({
                        "text": text,
                        "links": links
                    })
                
                if row_data:
                    table_data["rows"].append(row_data)
            
            if table_data["rows"]:
                data["tables"].append(table_data)
        
        # Find all links with addresses (Polygon addresses)
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link['href']
            text = link.get_text(strip=True)
            # Check if it's an address (starts with 0x)
            if href.startswith('http') and '0x' in text or href.startswith('0x'):
                data["links"].append({
                    "text": text,
                    "href": href
                })
        
        driver.save_screenshot("04_final.png")
        print("\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ:")
        print(f"  - –¢–∞–±–ª–∏—Ü: {len(data['tables'])}")
        print(f"  - –°—Å—ã–ª–æ–∫: {len(data['links'])}")
        
        return data
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        driver.save_screenshot("error.png")
        raise
        
    finally:
        print("\n‚è≥ –ë—Ä–∞—É–∑–µ—Ä –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—ã–º 60 —Å–µ–∫—É–Ω–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏...")
        print("   –í—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã")
        time.sleep(60)
        print("\n‚è≥ –ó–∞–∫—Ä—ã–≤–∞—é –±—Ä–∞—É–∑–µ—Ä...")
        driver.quit()


def main():
    print("\n" + "=" * 60)
    print("üåä HashDive Insiders Scraper")
    print("Using undetected-chromedriver")
    print("=" * 60)
    
    data = scrape_hashdive()
    
    if data:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"hashdive_data_{timestamp}.json"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        
        print("\nüìä –ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê:")
        print("=" * 60)
        
        if data['tables']:
            for table in data['tables']:
                print(f"\nüìã –¢–∞–±–ª–∏—Ü–∞ #{table['index']}:")
                print(f"   –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(table['headers'])}")
                if table['headers']:
                    print(f"   –ó–∞–≥–æ–ª–æ–≤–∫–∏: {', '.join(table['headers'][:5])}")
                print(f"   –°—Ç—Ä–æ–∫: {len(table['rows'])}")
                
                if table['rows']:
                    first_row = table['rows'][0]
                    if first_row:
                        print(f"   –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞: {first_row[0]['text'] if first_row else 'N/A'}")
                        if first_row[0]['links']:
                            print(f"   –°—Å—ã–ª–∫–∏: {len(first_row[0]['links'])}")
        else:
            print("\n‚ö†Ô∏è  –¢–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            print("\n–¢–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):")
            print(data['all_text'][:500])
        
        if data['links']:
            print(f"\nüîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(data['links'])}")
            for link in data['links'][:5]:
                print(f"   - {link['text']}: {link['href']}")
        
        print("\n" + "=" * 60)
        print(f"\nüí° –û—Ç–∫—Ä–æ–π—Ç–µ {filename} —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")


if __name__ == "__main__":
    main()

